#####################################################
#
# OSARIS configuration template
#
####################################################


# - - - - - GENERAL - - - - - - - - - - - - - - - - - 

debug=1
# Debug level
# 0 - none, 1 - moderate, 2 - maximum output

prefix="my_prefix"
# Used to group all data of the run.
# Typically use the name of the study region, e.g. "central-alps"

SAR_sensor="Sentinel"
SAR_datatype="TOPS"
swaths_to_process=(1 3)


# - - - - - BASE DATA PATHES - - - - - - - - - - - - - - -

base_PATH="/my/path/OSARIS-results/"
# Path to directory where OSARIS will put all files

input_files="/my/path/S1-scenes/"
# Options:
# download - use the DHuS script to obtain data, configure, below
# full path to a directory containing the S1 scene to process, e.g. /data/S1-scenes 

topo_PATH="/my/path/DEM/"
# Path to directory containing topo data (dem.grd file)
# Generate at http://topex.ucsd.edu/gmtsar/demgen/

orbits_PATH="/path/to/orbits_folder"
# Path to S1 orbit data
# Existing orbits will be used
# If folder does not exist or is empty, orbits will be downloaded here
# --> make sure to set $update_orbits to "1"


# - - - - - PROCESSING CONFIGURATION - - - - - - - - - - - - - - - - 

process_intf_mode=pairs
# Mode to process interferograms.
# pairs -> Chronologicallay moving pairs (A-B, B-C, C-D, ...)
# single_master -> Fixed master (A-B, A-C, A-D, ...)
# both -> process in both modes

# master_scene_date=20160217
# Optionally specify the scene to use as Master in single_master mode

process_pdf_overview=1
# Create an overview pdf of the processing results (0 = no,1 = yes)
# The PDF will be located in the output folder


# - - - - - MODULES - - - - - - - - - - - - - - - - 
# Make sure that for each acitvated module a configuration file 
# 'module_name.config' is located in the config folder. Copy the 
# template file from the module folder and fit it to your needs.

# post_download_mods=( ping )
# Modules to be run after downloading the S1 datasets

# post_extract_mods=( )
# Modules to be run after extracting the data S1 scenes

# post_processing_mods=( coherence_diff )
# Modules to be run after extracting the data

# post_postprocessing_mods=( )
# Modules to be run after calculating statistics and preparing summary reports



# - - - - - SLURM CONFIGURATION - - - - - - - - - - - - - -

slurm_account=your_account
# SLURM account name

slurm_ntasks=5
# Number of cores used for parallel processing. 
# Fit this number to make sure sufficient RAM (~15 GB for full S1 swath) is available for processing.
# For example, when 3 GB of RAM are allocated for each core, use 5 or more ntasks.

slurm_jobname_prefix="Prefix"
# Choose a name to identify your jobs in the SLURM queue

slurm_qos=your_qos
# Job type

slurm_partition=your_partition
# Partition used for computing.
# Optional in most cases, comment out when not needed

# slurm_extract_qos=your_extract_qos
# Job type for extract jobs
# Optional, only required if different from other processing, 
# e.g. because of unzip is not installed on processing nodes

# slurm_extract_partition=your_extract_partition
# Partition used for computing.
# Optional, only required by Slurm config and when different from partition defined with 'slurm_partition'

slurm_mailtype=ALL
# SLURM mail setup. 
# Optional in most cases, comment out when not needed



# - - - - - DOWNLOADS AND DATA - - - - - - - - - - - - 

update_orbits=1
# Activate (1) or deactivate (0) automatic orbit retrieval

orig_files="extract"
# Options:
# extract - Unizp files from input folder
# keep - recommended when files were already extracted in a previous run

# username="your_dhus_username"
# password="your_dhus_password"
# Login credentials for your ESA DHuS account
# Only required when $input_files is set to "download"

# import_data_type="search_string"
# Configuration for DHuS download 
# (only relevant when 'input_files' is set to 'download')
# Options:
# meta4 - use meta4 file obtained from the DHuS website
# filelist - use a list file generated according to DHuS rules (e.g. from a previous GSP run)
# search_string - use a dhus-comapatible search string

# meta4_file="/path/to/meta4_data/yourfile.meta4"
# Path and file name of meta4 file
# (only relevant when 'input_files' is set to 'download' and 'import_data' to 'meta4')
# (a) Path and file name of a .meta4 file

# filelist_file="/path/to/filelists/your_filelist.txt"
# Path and file name of filelist file
# (only relevant when 'input_files' is set to 'download')
# Either generated from meta4 file or allready existing

# If not using meta4 or file list, specifiy the DHuSget search configuration manually 
# download_string="-m Sentinel-1 -i SAR -S 2015-10-01T06:00:00.000Z -c 74.330,42.420:74.620,46.600 -T SLC"
# download_string="-F '( footprint:\"Intersects(POLYGON((74.10939131201312 42.27639698758162,74.81882318407474 42.27639698758162,74.81882318407474 42.635216267951876,74.10939131201312 42.635216267951876,74.10939131201312 42.27639698758162)))\" ) AND ( beginPosition:[2015-05-01T00:00:00.000Z TO NOW] AND endPosition:[2015-05-01T00:00:00.000Z TO NOW] ) AND (platformname:Sentinel-1 AND producttype:SLC AND sensoroperationalmode:IW AND relativeorbitnumber:27 AND slicenumber:11 )'" 

# download_option="product"
# Options: product - manifest - all

# concurrent_downloads=2

clean_up=0
# Delete files used for processing after finishing the job
# 0 or not set -> Keep all files
# 1 -> Delete files solely used for this run (will keep extracted S1 files for subsequent jobs
# 2 -> Delete the whole processing folder


# - - - - - GMTSAR CONFIG FILE - - - - - - - - - - 

# File name of GMTSAR congif file. Specify full path if the file is
# not located in the program directory (along with this config file)
gmtsar_config_file="config/GMTSAR-default.config"

